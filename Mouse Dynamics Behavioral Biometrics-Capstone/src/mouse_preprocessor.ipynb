{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e363cb3d-f484-495b-bb1c-14bc23827cd3",
      "metadata": {
        "id": "e363cb3d-f484-495b-bb1c-14bc23827cd3"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "#  Importing necessary libraries\n",
        "import numpy as np\n",
        "#  Importing necessary libraries\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3e41f58e-17e6-4b3b-b2ef-62f3198f9a43",
      "metadata": {
        "id": "3e41f58e-17e6-4b3b-b2ef-62f3198f9a43"
      },
      "outputs": [],
      "source": [
        "def preprocessor(path, user_col, x_col, y_col, e_col, t_col, subfolder_users=False):\n",
        "    # Initialize an empty list to collect DataFrames from each file\n",
        "    all_data = []\n",
        "    user = 0  # Counter to assign unique user IDs if not found in file\n",
        "    sess_col = 'Session'\n",
        "\n",
        "    #  If data is stored in subfolders (one per user)\n",
        "    if subfolder_users:\n",
        "        for subfolder in os.listdir(path):\n",
        "            sess = 0  # Initialize session count for each user\n",
        "            subfolder_path = os.path.join(path, subfolder)\n",
        "\n",
        "            # Skip if it's not a directory\n",
        "            if not os.path.isdir(subfolder_path):\n",
        "                continue\n",
        "\n",
        "            for file in os.listdir(subfolder_path):\n",
        "                print(subfolder, file)\n",
        "                file_path = os.path.join(subfolder_path, file)\n",
        "\n",
        "                # Skip if it's not a file\n",
        "                if not os.path.isfile(file_path):\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # ðŸ“¥ Reading a CSV file into a DataFrame\n",
        "                    df = pd.read_csv(file_path)\n",
        "                except pd.errors.ParserError as e:\n",
        "                    print('ParserError in file: ', file_path)\n",
        "                    print(e)\n",
        "                    continue\n",
        "\n",
        "                #  Remove duplicates and sort by timestamp\n",
        "                df = df.drop_duplicates(subset=t_col, keep='first')\n",
        "                df = df.sort_values(by=t_col).reset_index(drop=True)\n",
        "\n",
        "                # If user_col not provided or missing in file, add it manually\n",
        "                if not user_col or user_col not in df.columns:\n",
        "                    df['User'] = [user] * len(df)\n",
        "                    user_col = 'User'\n",
        "\n",
        "                # Add a session number column\n",
        "                df['Session'] = sess\n",
        "\n",
        "                # Store this DataFrame\n",
        "                all_data.append(df)\n",
        "                sess += 1  # Move to next session\n",
        "            user += 1  # Move to next user\n",
        "\n",
        "    #  If all CSVs are in a flat folder (no subfolders)\n",
        "    else:\n",
        "        for file in os.listdir(path):\n",
        "            file_path = os.path.join(path, file)\n",
        "\n",
        "            if not os.path.isfile(file_path):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # ðŸ“¥ Reading a CSV file into a DataFrame\n",
        "                df = pd.read_csv(file_path)\n",
        "            except Exception as e:\n",
        "                print('ParserError in file: ', file_path)\n",
        "                print(e)\n",
        "                continue\n",
        "\n",
        "            # Add a synthetic 'User' column if missing\n",
        "            if not user_col or user_col not in df.columns:\n",
        "                df['User'] = [user] * len(df)\n",
        "                user_col = 'User'\n",
        "\n",
        "            all_data.append(df)\n",
        "        user += 1\n",
        "\n",
        "    #  Combine all session data into one DataFrame\n",
        "    all_data = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "    #  Keep only the relevant columns and standardize column names\n",
        "    cols = [user_col, sess_col, x_col, y_col, e_col, t_col]\n",
        "    all_data = all_data[cols]\n",
        "    all_data.columns = ['User', 'Session', 'X', 'Y', 'Event', 'Timestamp']\n",
        "\n",
        "    return all_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "fde8cc58-343e-4da2-9ab2-c9cc8ff2e5ec",
      "metadata": {
        "id": "fde8cc58-343e-4da2-9ab2-c9cc8ff2e5ec"
      },
      "outputs": [],
      "source": [
        "def perception_windows(data):\n",
        "    # Initialize storage for training and validation sequences, labels, lengths, and raw (X, Y, Timestamp) data\n",
        "    X_train = []\n",
        "    X_val = []\n",
        "    y_train = []\n",
        "    y_val = []\n",
        "    lens = []\n",
        "    raw_X = []\n",
        "\n",
        "    # Total number of users in the dataset\n",
        "    total_users = len(np.unique(data['User']))\n",
        "\n",
        "    # Loop through each user\n",
        "    for user in tqdm(np.unique(data['User'])):\n",
        "        # Filter data for the current user\n",
        "        user_X = data[data['User'] == user]\n",
        "\n",
        "        # Store valid trajectories and raw trajectories for this user\n",
        "        user_valid_trajs = []\n",
        "        user_raw_X = []\n",
        "\n",
        "        # Loop through each session of the current user\n",
        "        for sess in np.unique(user_X['Session']):\n",
        "            # Extract session-specific data\n",
        "            sess_X = user_X[user_X['Session'] == sess].copy()\n",
        "            raw_sess_X = sess_X[['X', 'Y', 'Timestamp']].copy()\n",
        "\n",
        "            # Compute time difference between events\n",
        "            time_diff = sess_X['Timestamp'].diff().fillna(0)\n",
        "\n",
        "            # Find break points where pause between events >= 250 ms\n",
        "            p_idx = time_diff >= 250\n",
        "\n",
        "            # Compute derivatives (Î”X, Î”Y, Î”T)\n",
        "            sess_X[['Delta_X', 'Delta_Y', 'Delta_T']] = raw_sess_X[['X', 'Y', 'Timestamp']].diff().fillna(0)\n",
        "\n",
        "            # Reset the deltas at pause points to prevent carry-over across split\n",
        "            sess_X.loc[p_idx, ['Delta_X', 'Delta_Y', 'Delta_T']] = 0\n",
        "\n",
        "            # Keep only delta features\n",
        "            sess_X = sess_X[['Delta_X', 'Delta_Y', 'Delta_T']]\n",
        "\n",
        "            # Split session into trajectory segments using the pause indices\n",
        "            segs = np.split(sess_X.values, np.where(p_idx)[0])\n",
        "            segs_raw = np.split(raw_sess_X.values, np.where(p_idx)[0])\n",
        "\n",
        "            # Keep only segments with at least 5 points\n",
        "            user_valid_trajs.extend([window for window in segs if len(window) >= 5])\n",
        "            user_raw_X.extend([window for window in segs_raw if len(window) >= 5])\n",
        "\n",
        "        # Filter out users with too few valid trajectories\n",
        "        if int(len(user_valid_trajs) * 0.2) < total_users - 1:\n",
        "            continue\n",
        "\n",
        "        # Split user's trajectories into 80% training and 20% validation\n",
        "        split_idx = int(len(user_valid_trajs) * 0.8)\n",
        "\n",
        "        # Add to global dataset\n",
        "        X_train.extend(user_valid_trajs[:split_idx])\n",
        "        y_train.extend([user] * len(user_valid_trajs[:split_idx]))\n",
        "        X_val.extend(user_valid_trajs[split_idx:])\n",
        "        y_val.extend([user] * len(user_valid_trajs[split_idx:]))\n",
        "        raw_X.extend(user_raw_X[:split_idx])\n",
        "        lens.extend([len(compat_seg) for compat_seg in user_valid_trajs])\n",
        "\n",
        "    # Return training and validation sets, along with sequence lengths and raw XYT data\n",
        "    return X_train, y_train, X_val, y_val, lens, raw_X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "2500d016-612a-4d18-8fa1-2844e392f9bb",
      "metadata": {
        "id": "2500d016-612a-4d18-8fa1-2844e392f9bb"
      },
      "outputs": [],
      "source": [
        "def filter_outliers(lens, X):\n",
        "    #  Print the min and max sequence lengths before filtering\n",
        "    print(np.min(lens), np.max(lens))\n",
        "\n",
        "    # Convert the list of lengths to a NumPy array for processing\n",
        "    lens = np.array(lens)\n",
        "\n",
        "    #  Sort the lengths and keep their original indices\n",
        "    lens_sort_inds = np.argsort(lens)\n",
        "    lens_sorted = lens[lens_sort_inds]\n",
        "\n",
        "    # Calculate Q1 and Q3 (25th and 75th percentiles) using midpoint method\n",
        "    q1_lens = np.percentile(lens_sorted, 25, method='midpoint')\n",
        "    q3_lens = np.percentile(lens_sorted, 75, method='midpoint')\n",
        "\n",
        "    # Print Q1 and Q3 for inspection\n",
        "    print(q1_lens, q3_lens)\n",
        "\n",
        "    #  Compute the interquartile range (IQR)\n",
        "    IQR_lens = q3_lens - q1_lens\n",
        "\n",
        "    # Define lower and upper bounds using IQR (Tukey's method)\n",
        "    low_lim_lens = q1_lens - 1.5 * IQR_lens\n",
        "    up_lim_lens = q3_lens + 1.5 * IQR_lens\n",
        "\n",
        "    # Print the bounds for filtering\n",
        "    print(low_lim_lens, up_lim_lens)\n",
        "\n",
        "    #  Create a mask to filter out lengths outside [low_lim, up_lim]\n",
        "    mask = (low_lim_lens < lens_sorted) & (lens_sorted < up_lim_lens)\n",
        "\n",
        "    # Reorder the mask to match the original unsorted indices\n",
        "    mask = mask[np.argsort(lens_sort_inds)]\n",
        "\n",
        "    # Apply the mask to remove outliers from the length list\n",
        "    lens = lens[mask]\n",
        "\n",
        "    #  Print how many lengths remain after filtering and their mean\n",
        "    print(len(lens))\n",
        "    print(np.mean(lens))\n",
        "\n",
        "    #  Set minimum and maximum lengths to keep for zero padding/truncation\n",
        "    bottom_n = int(min(lens))  # lower bound (used as min sequence length)\n",
        "    top_n = int(max(lens))     # upper bound (used as max sequence length)\n",
        "\n",
        "    return bottom_n, top_n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "08b0492d-7b16-4a84-b1ae-8ed5a392bf50",
      "metadata": {
        "id": "08b0492d-7b16-4a84-b1ae-8ed5a392bf50"
      },
      "outputs": [],
      "source": [
        "def zero_pad(X, y):\n",
        "    # Temporary lists to store filtered and valid sequences and labels\n",
        "    tmp_X = []\n",
        "    tmp_y = []\n",
        "    traj_list = []  # This list is unused and can be removed if not used later\n",
        "\n",
        "    # Initialize a zero-padded 3D NumPy array to hold all padded sequences\n",
        "    # Shape: (number of sequences, max allowed length, 3 features per timestep)\n",
        "    outs = np.zeros((len(X), top_n, 3))\n",
        "\n",
        "    a = 0  # Index for tracking how many valid sequences are added\n",
        "\n",
        "    # Loop through all sequences with their labels\n",
        "    for i, item in tqdm(enumerate(X), total=len(X)):\n",
        "        # Only include sequences longer than the bottom threshold\n",
        "        if len(item) > bottom_n:\n",
        "            # If sequence is shorter than top_n, pad it on the left (right-aligned)\n",
        "            if len(item) < top_n:\n",
        "                needed_zeros = top_n - len(item)\n",
        "                tmp_y.append(y[i])\n",
        "                outs[a, -len(item):, :] = item\n",
        "            else:\n",
        "                # If sequence is too long, truncate it from the front\n",
        "                outs[a, -len(item):, :] = item[:top_n]\n",
        "                tmp_y.append(y[i])\n",
        "            a += 1\n",
        "\n",
        "    # Slice the padded output array to include only the filled rows\n",
        "    X = outs[:len(tmp_y)]\n",
        "    y = tmp_y\n",
        "\n",
        "    # Print how many sequences are retained\n",
        "    print(len(X))\n",
        "\n",
        "    # Convert to NumPy arrays\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "5e44bbac-42c2-4b45-b4c0-0f50e61e454b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e44bbac-42c2-4b45-b4c0-0f50e61e454b",
        "outputId": "3a869e72-c1c3-4c1c-954b-94f771e95b67",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9800000 session_0.csv\n",
            "9800000 session_2.csv\n",
            "9800000 session_1.csv\n",
            "9800000 session_3.csv\n",
            "9800000 session_6.csv\n",
            "9800000 session_5.csv\n",
            "9800000 session_4.csv\n",
            "9911071 session_0.csv\n",
            "9911071 session_5.csv\n",
            "9911071 session_4.csv\n",
            "9911071 session_3.csv\n",
            "9911071 session_1.csv\n",
            "9911071 session_2.csv\n",
            "9779049 session_2.csv\n",
            "9779049 session_0.csv\n",
            "9779049 session_7.csv\n",
            "9779049 session_3.csv\n",
            "9779049 session_4.csv\n",
            "9779049 session_8.csv\n",
            "9779049 session_6.csv\n",
            "9779049 session_1.csv\n",
            "9779049 session_5.csv\n",
            "9609325 session_4.csv\n",
            "9609325 session_2.csv\n",
            "9609325 session_0.csv\n",
            "9609325 session_6.csv\n",
            "9609325 session_1.csv\n",
            "9609325 session_8.csv\n",
            "9609325 session_11.csv\n",
            "9609325 session_5.csv\n",
            "9609325 session_3.csv\n",
            "9609325 session_7.csv\n",
            "9609325 session_10.csv\n",
            "9609325 session_9.csv\n",
            "9031166 session_4.csv\n",
            "9031166 session_5.csv\n",
            "9031166 session_6.csv\n",
            "9031166 session_2.csv\n",
            "9031166 session_0.csv\n",
            "9031166 session_3.csv\n",
            "9031166 session_1.csv\n",
            "9031166 session_7.csv\n",
            "9031166 session_9.csv\n",
            "9031166 session_8.csv\n",
            "8058937 session_10.csv\n",
            "8058937 session_0.csv\n",
            "8058937 session_2.csv\n",
            "8058937 session_1.csv\n",
            "8058937 session_3.csv\n",
            "8058937 session_9.csv\n",
            "8058937 session_7.csv\n",
            "8058937 session_8.csv\n",
            "8058937 session_6.csv\n",
            "8058937 session_5.csv\n",
            "8058937 session_4.csv\n",
            "8139946 session_1.csv\n",
            "8139946 session_0.csv\n",
            "8139946 session_7.csv\n",
            "8139946 session_2.csv\n",
            "8139946 session_6.csv\n",
            "8139946 session_8.csv\n",
            "8139946 session_10.csv\n",
            "8139946 session_5.csv\n",
            "8139946 session_4.csv\n",
            "8139946 session_3.csv\n",
            "8139946 session_9.csv\n",
            "8941269 session_12.csv\n",
            "8941269 session_3.csv\n",
            "8941269 session_11.csv\n",
            "8941269 session_1.csv\n",
            "8941269 session_6.csv\n",
            "8941269 session_10.csv\n",
            "8941269 session_2.csv\n",
            "8941269 session_5.csv\n",
            "8941269 session_0.csv\n",
            "8941269 session_4.csv\n",
            "8941269 session_8.csv\n",
            "8941269 session_9.csv\n",
            "8941269 session_7.csv\n",
            "9274054 session_0.csv\n",
            "9274054 .ipynb_checkpoints\n",
            "9604161 session_4.csv\n",
            "9604161 session_0.csv\n",
            "9604161 session_3.csv\n",
            "9604161 session_1.csv\n",
            "9604161 session_2.csv\n",
            "7921384 session_0.csv\n",
            "7882967 session_0.csv\n",
            "7547835 session_0.csv\n",
            "7547835 session_2.csv\n",
            "7547835 session_1.csv\n",
            "7547835 session_3.csv\n",
            "7462635 session_1.csv\n",
            "7462635 session_0.csv\n",
            "7462635 session_9.csv\n",
            "7462635 session_5.csv\n",
            "7462635 session_7.csv\n",
            "7462635 session_12.csv\n",
            "7462635 session_3.csv\n",
            "7462635 session_6.csv\n",
            "7462635 session_8.csv\n",
            "7462635 session_4.csv\n",
            "7462635 session_2.csv\n",
            "7462635 session_10.csv\n",
            "7462635 session_11.csv\n",
            "7462635 session_13.csv\n",
            "7058298 session_1.csv\n",
            "7058298 session_4.csv\n",
            "7058298 session_0.csv\n",
            "7058298 session_3.csv\n",
            "7058298 session_2.csv\n",
            "7058298 session_7.csv\n",
            "7058298 session_6.csv\n",
            "7058298 session_5.csv\n",
            "7642950 session_13.csv\n",
            "7642950 session_11.csv\n",
            "7642950 session_15.csv\n",
            "7642950 session_16.csv\n",
            "7642950 session_0.csv\n",
            "7642950 session_10.csv\n",
            "7642950 session_14.csv\n",
            "7642950 session_12.csv\n",
            "7642950 session_1.csv\n",
            "7642950 session_17.csv\n",
            "7642950 session_19.csv\n",
            "7642950 session_26.csv\n",
            "7642950 session_18.csv\n",
            "7642950 session_22.csv\n",
            "7642950 session_4.csv\n",
            "7642950 session_20.csv\n",
            "7642950 session_21.csv\n",
            "7642950 session_23.csv\n",
            "7642950 session_3.csv\n",
            "7642950 session_2.csv\n",
            "7642950 session_25.csv\n",
            "7642950 session_24.csv\n",
            "7642950 session_9.csv\n",
            "7642950 session_8.csv\n",
            "7642950 session_6.csv\n",
            "7642950 session_7.csv\n",
            "7642950 session_5.csv\n",
            "7164913 session_0.csv\n",
            "7164913 session_2.csv\n",
            "7164913 session_6.csv\n",
            "7164913 session_1.csv\n",
            "7164913 session_3.csv\n",
            "7164913 session_5.csv\n",
            "7164913 session_4.csv\n",
            "7495794 session_0.csv\n",
            "6973075 session_3.csv\n",
            "6973075 session_0.csv\n",
            "6973075 session_2.csv\n",
            "6973075 session_4.csv\n",
            "6973075 session_1.csv\n",
            "6973075 session_5.csv\n",
            "6973075 session_6.csv\n",
            "6973075 session_9.csv\n",
            "6973075 session_7.csv\n",
            "6973075 session_8.csv\n",
            "7030676 session_0.csv\n",
            "7030676 session_4.csv\n",
            "7030676 session_1.csv\n",
            "7030676 session_3.csv\n",
            "7030676 session_2.csv\n",
            "6755264 session_2.csv\n",
            "6755264 session_6.csv\n",
            "6755264 session_1.csv\n",
            "6755264 session_4.csv\n",
            "6755264 session_3.csv\n",
            "6755264 session_5.csv\n",
            "6755264 session_0.csv\n",
            "6485798 session_0.csv\n",
            "6485798 session_1.csv\n",
            "6485798 session_3.csv\n",
            "6485798 session_2.csv\n",
            "6425139 session_0.csv\n",
            "6414470 session_3.csv\n",
            "6414470 session_2.csv\n",
            "6414470 session_1.csv\n",
            "6414470 session_0.csv\n",
            "4906875 session_1.csv\n",
            "4906875 session_0.csv\n",
            "4906875 session_2.csv\n",
            "5563756 session_4.csv\n",
            "5563756 session_3.csv\n",
            "5563756 session_2.csv\n",
            "5563756 session_5.csv\n",
            "5563756 session_0.csv\n",
            "5563756 session_1.csv\n",
            "5987580 session_0.csv\n",
            "6086694 session_0.csv\n",
            "6086694 session_5.csv\n",
            "6086694 session_4.csv\n",
            "6086694 session_3.csv\n",
            "6086694 session_1.csv\n",
            "6086694 session_2.csv\n",
            "5389326 session_1.csv\n",
            "5389326 session_0.csv\n",
            "5389326 session_2.csv\n",
            "5389326 session_3.csv\n",
            "5125125 session_7.csv\n",
            "5125125 session_2.csv\n",
            "5125125 session_0.csv\n",
            "5125125 session_6.csv\n",
            "5125125 session_1.csv\n",
            "5125125 session_4.csv\n",
            "5125125 session_3.csv\n",
            "5125125 session_5.csv\n",
            "5125125 session_8.csv\n",
            "4376779 session_0.csv\n",
            "4376779 session_3.csv\n",
            "4376779 session_1.csv\n",
            "4376779 session_2.csv\n",
            "4376779 session_4.csv\n",
            "4376779 session_5.csv\n",
            "4376779 session_6.csv\n",
            "4064487 session_6.csv\n",
            "4064487 session_3.csv\n",
            "4064487 session_0.csv\n",
            "4064487 session_2.csv\n",
            "4064487 session_5.csv\n",
            "4064487 session_1.csv\n",
            "4064487 session_4.csv\n",
            "3604291 session_1.csv\n",
            "3604291 session_0.csv\n",
            "3667465 session_1.csv\n",
            "3667465 session_3.csv\n",
            "3667465 session_0.csv\n",
            "3667465 session_2.csv\n",
            "2884292 session_13.csv\n",
            "2884292 session_10.csv\n",
            "2884292 session_15.csv\n",
            "2884292 session_3.csv\n",
            "2884292 session_12.csv\n",
            "2884292 session_0.csv\n",
            "2884292 session_11.csv\n",
            "2884292 session_17.csv\n",
            "2884292 session_16.csv\n",
            "2884292 session_1.csv\n",
            "2884292 session_2.csv\n",
            "2884292 session_14.csv\n",
            "2884292 session_7.csv\n",
            "2884292 session_5.csv\n",
            "2884292 session_4.csv\n",
            "2884292 session_6.csv\n",
            "2884292 session_8.csv\n",
            "2884292 session_9.csv\n",
            "2330116 session_0.csv\n",
            "2330116 session_2.csv\n",
            "2330116 session_4.csv\n",
            "2330116 session_1.csv\n",
            "2330116 session_3.csv\n",
            "2330116 session_5.csv\n",
            "2164619 session_2.csv\n",
            "2164619 session_0.csv\n",
            "2164619 session_1.csv\n",
            "2164619 session_3.csv\n",
            "2164619 session_4.csv\n",
            "2057188 session_0.csv\n",
            "2057188 session_2.csv\n",
            "2057188 session_1.csv\n",
            "2451411 session_5.csv\n",
            "2451411 session_2.csv\n",
            "2451411 session_4.csv\n",
            "2451411 session_1.csv\n",
            "2451411 session_6.csv\n",
            "2451411 session_3.csv\n",
            "2451411 session_0.csv\n",
            "2451411 session_8.csv\n",
            "2451411 session_7.csv\n",
            "2481013 session_1.csv\n",
            "2481013 session_11.csv\n",
            "2481013 session_10.csv\n",
            "2481013 session_0.csv\n",
            "2481013 session_7.csv\n",
            "2481013 session_6.csv\n",
            "2481013 session_4.csv\n",
            "2481013 session_2.csv\n",
            "2481013 session_5.csv\n",
            "2481013 session_3.csv\n",
            "2481013 session_9.csv\n",
            "2481013 session_8.csv\n",
            "1513190 session_0.csv\n",
            "1513190 session_1.csv\n",
            "1513190 session_5.csv\n",
            "1513190 session_2.csv\n",
            "1513190 session_4.csv\n",
            "1513190 session_3.csv\n",
            "1514461 session_0.csv\n",
            "1067144 session_0.csv\n"
          ]
        }
      ],
      "source": [
        "# Run the custom preprocessor function to load and clean mouse movement data\n",
        "# Parameters:\n",
        "# - Path to the dataset\n",
        "# - 'None' for user_col, so it assigns synthetic user IDs\n",
        "# - Column names for X, Y, event type, and timestamp\n",
        "# - subfolder_users=True indicates that each user's data is stored in a separate subfolder\n",
        "data = preprocessor('/content/drive/MyDrive/Karthik MS DS Capstone/Gmail Dataset Formatted/mouse/',\n",
        "                    None, 'Mouse X', 'Mouse Y', 'Type', 'TimeStamp', True)\n",
        "\n",
        "# Optional: Save the full cleaned data as a CSV file for inspection\n",
        "# data.to_csv('Gmail.csv', index=False)\n",
        "\n",
        "# Optional: Save the full cleaned data as a NumPy array for future use\n",
        "# np.save('Gmail.npy', data.to_numpy())\n",
        "\n",
        "# Ensure the important numeric columns are properly converted from strings (if needed)\n",
        "cols_to_fix = ['X', 'Y', 'Timestamp']\n",
        "for col in cols_to_fix:\n",
        "    # Convert column to numeric, setting non-convertible values to NaN\n",
        "    data[col] = pd.to_numeric(data[col], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c98b7db9-64d6-4768-9f26-bdba6c857230",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c98b7db9-64d6-4768-9f26-bdba6c857230",
        "outputId": "10e08313-0097-4d1c-8284-650d8cf7caac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:03<00:00, 11.79it/s]\n"
          ]
        }
      ],
      "source": [
        "# Convert the 'Timestamp' column to numeric values\n",
        "# Any non-numeric or malformed values will be set to NaN (coerced)\n",
        "data['Timestamp'] = pd.to_numeric(data['Timestamp'], errors='coerce')\n",
        "\n",
        "# Process the cleaned DataFrame into sequences suitable for LSTM modeling\n",
        "# This function performs:\n",
        "# - User-wise and session-wise segmentation\n",
        "# - Trajectory extraction using 250ms pause detection\n",
        "# - Derivative computation (Î”X, Î”Y, Î”T)\n",
        "# - Sequence filtering (length â‰¥ 5)\n",
        "# - 80/20 train-validation split per user\n",
        "# Returns:\n",
        "# - X_train, y_train: list of padded input sequences and their labels (train set)\n",
        "# - X_val, y_val: same for validation set\n",
        "# - lens: list of lengths of all sequences\n",
        "# - raw_X: original X/Y/T data for visualization or further analysis\n",
        "X_train, y_train, X_val, y_val, lens, raw_X = perception_windows(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1551d723-b7c5-481d-99dc-740aa68a1331",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1551d723-b7c5-481d-99dc-740aa68a1331",
        "outputId": "a99f82d7-4dfd-42de-a52f-1d01c5d7fb7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "93935 23502\n"
          ]
        }
      ],
      "source": [
        "print(len(X_train), len(X_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4149907d-0adb-422f-8199-bef4554d12cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4149907d-0adb-422f-8199-bef4554d12cb",
        "outputId": "bef26519-e232-4c45-d2a9-d39612328675"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5 950\n",
            "14.0 47.0\n",
            "-35.5 96.5\n",
            "110059\n",
            "30.387401302937516\n",
            "5 96\n"
          ]
        }
      ],
      "source": [
        "# Apply interquartile range (IQR) based filtering to remove outlier sequences\n",
        "# from the combined training and validation sets based on their lengths\n",
        "# This function returns:\n",
        "# - bottom_n: the minimum sequence length threshold (for padding/truncation)\n",
        "# - top_n: the maximum sequence length threshold\n",
        "bottom_n, top_n = filter_outliers(lens, X_train + X_val)\n",
        "\n",
        "# Print the selected lower and upper bounds for sequence length\n",
        "print(bottom_n, top_n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "44f82026-58f1-4805-be7d-39b773abfe67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44f82026-58f1-4805-be7d-39b773abfe67",
        "outputId": "3874162d-152a-44b5-b756-f84effed97fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91482/91482 [00:00<00:00, 289380.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "91482\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22889/22889 [00:00<00:00, 293922.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22889\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91482/91482 [00:00<00:00, 127686.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "91482\n"
          ]
        }
      ],
      "source": [
        "# Pad or truncate each training sequence to have the same length (top_n)\n",
        "# Only sequences longer than bottom_n are retained\n",
        "# Resulting shape: (num_sequences, top_n, 3), where 3 = Î”X, Î”Y, Î”T\n",
        "X_train, y_train = zero_pad(X_train, y_train)\n",
        "\n",
        "# Apply the same zero-padding/truncation process to the validation set\n",
        "X_val, y_val = zero_pad(X_val, y_val)\n",
        "\n",
        "# Apply zero-padding to raw (X, Y, Timestamp) sequences for visualization or auxiliary evaluation\n",
        "# Since these don't have class labels, a dummy label array of ones is passed\n",
        "raw_X, _ = zero_pad(raw_X, np.ones(len(raw_X)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d69364a-94b6-4629-af3b-6a8cf136e924",
      "metadata": {
        "id": "9d69364a-94b6-4629-af3b-6a8cf136e924"
      },
      "outputs": [],
      "source": [
        "# Save the padded raw (X, Y, Timestamp) sequences for visualization or validation use\n",
        "\n",
        "np.save('../data/data_splits/multiclass/raw_X.npy', raw_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c77a60-3f7d-4fae-b833-b12b7aea86f6",
      "metadata": {
        "id": "a2c77a60-3f7d-4fae-b833-b12b7aea86f6"
      },
      "outputs": [],
      "source": [
        "# Save the padded training sequences and corresponding labels for multiclass classification\n",
        "#replace the first argument with the path where you want to save the preprocessed data\n",
        "np.save('../data/data_splits/multiclass/X_train_event.npy', X_train)\n",
        "np.save('../data/data_splits/multiclass/y_train_event.npy', y_train)\n",
        "\n",
        "# Save the padded validation sequences and corresponding labels\n",
        "np.save('../data/data_splits/multiclass/X_val_event.npy', X_val)\n",
        "np.save('../data/data_splits/multiclass/y_val_event.npy', y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dt3n3of2Qu_B",
      "metadata": {
        "id": "dt3n3of2Qu_B"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Save each user's padded sequences for binary classification\n",
        "save_path = \"..data/data splits/binary\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save padded sequences per user\n",
        "for user_id in np.unique(y_train):\n",
        "    user_sequences = X_train[y_train == user_id]\n",
        "    np.save(f\"{save_path}{user_id}.npy\", user_sequences)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
